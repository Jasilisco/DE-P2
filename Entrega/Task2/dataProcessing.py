import pandas as pd
import dataTransformations as tfm
import dataCleaning as cln
from config import cfg
import dbUtils as db
import os

def processDatasources(chunksize):
    base = ""
    remove = True
    for _, data in cfg.datasources.items():
        if isinstance(data, dict) and data.get("base", False):
            remove = False
            base = data.get("name", False)
        else:
            continue
    for key, data in cfg.datasources.items():
        if isinstance(data, dict) and (data.get("name", "") != base):
            print(f"[INFO] Merging file {key} with {base}")
            base = mergeCSV(
                data.get("name", ""),
                base,
                chunksize,
                data.get("mergeLogic", {}),
                data.get("rename", {}),
                key,
                remove=remove
            )
            remove = True
    return base

def mergeCSV(file1, file2, chunksize, mergeLogic, rename={}, resultSuffix="general", remove = True):
    file1DF = pd.read_csv(file1, low_memory=False)

    for old_name, new_name in rename.items():
        if old_name in file1DF.columns:
            file1DF = file1DF.rename(columns={old_name: new_name})

    left_col = mergeLogic.get("base")
    right_col = mergeLogic.get("field")
    file1DF[right_col] = file1DF[right_col].astype(str).str.strip()

    output_file = f"result_{resultSuffix}.csv"

    for i, chunk in enumerate(
        pd.read_csv(file2, chunksize=chunksize, low_memory=False)
    ):
        chunk[left_col] = chunk[left_col].astype(str).str.strip()

        merged = chunk.merge(
            file1DF,
            left_on=left_col,
            right_on=right_col,
            how="left",
            suffixes=("_old", None),
        )

        mode = "w" if i == 0 else "a"
        header = i == 0
        merged.to_csv(output_file, mode=mode, header=header, index=False)
    if remove:
        os.remove(file2)
    print(f"[INFO] Merge completed. Saved file in {output_file}")

    return output_file

def extractFromDimension(df, fk_config, dimensions):
    fk_dim = fk_config.get("fk")
    if not fk_dim or fk_dim not in dimensions:
        return pd.Series([None] * len(df), index=df.index)

    dim_df = pd.DataFrame(dimensions[fk_dim]).copy()
    aux_df = pd.DataFrame(df).copy()

    id_key = cfg.idMapping.get(fk_dim)
    if not id_key or id_key not in dim_df.columns:
        return pd.Series([None] * len(df), index=df.index)

    criteria = cfg.onDuplicateBase.get(fk_dim)
    if not criteria:
        return pd.Series([None] * len(df), index=df.index)
    if not isinstance(criteria, list):
        criteria = [criteria]
    
    criteriaLeft = [
        c if isinstance(cfg.tables[fk_dim].get(c), dict) and "value" in cfg.tables[fk_dim][c]
        else cfg.tables[fk_dim][c]
        for c in criteria
        if c in cfg.tables[fk_dim]
    ]
    criteriaRight = criteria

    for i, col_right in enumerate(criteriaRight):
        col_left = criteriaLeft[i]
        rule = cfg.tables[fk_dim].get(col_right)
        aux_df[col_left] = tfm.applyTransform(rule, aux_df, col_right)

    merged = aux_df.merge(
        dim_df,
        how="left",
        left_on=criteriaLeft,
        right_on=criteriaRight,
        suffixes=("", "_dim"),
        validate="m:1",
    )

    return merged.set_index(df.index)[id_key]

def generateRecords(df, table_cfg, dimensions = []):
    records = pd.DataFrame(index=df.index)
    for col_name, rules in table_cfg.items():
        if isinstance(rules, dict) and "value" in rules or isinstance(rules, str):
            temp = tfm.applyTransform(rules, df, col_name)
        elif isinstance(rules, dict) and "fk" in rules:
            temp = extractFromDimension(df, rules, dimensions)
        else: temp = None        
        if temp is not None:
            records[col_name] = temp
        else:
            print(f"[ERROR] Failed to process {col_name}") # Needs fixing to avoid logging autogenerated columns 
            continue
    # Ensure DB-safe nulls across all columns
    records = records.astype(object)
    records = records.where(pd.notna(records), None)
    return records

def mapRecords(df, result, dimensions = []):
    for table, table_cfg in cfg.tables.items():
        if table not in result:
            continue

        records = generateRecords(df, table_cfg, dimensions=dimensions)
        if not records.empty:
            result[table].extend(records.to_dict(orient="records"))
    return result

def extractFKDimensions():
    dimension = {}

    for table_name, table_cfg in cfg.tables.items():
        table_design = cfg.design.get(table_name, {})

        is_fact = (
            isinstance(table_design, dict) and table_design.get("value") == "fact"
        ) or table_design == "fact"

        if is_fact:
            for _, rules in table_cfg.items():
                if isinstance(rules, dict):
                    fk = rules.get("fk", False)
                    if fk and fk not in dimension:
                        try:
                            dimension[fk] = db.extractTable(fk)
                        except Exception as ex:
                            print(f"[WARNING] Could not extract dimension {fk}: {ex}")
                            dimension[fk] = []

    return dimension


def dataStructure(procedure):
    result = {}
    for table, _ in cfg.tables.items():
        if (
            isinstance(cfg.design[table], dict)
            and cfg.design[table]["value"] == procedure
        ):
            result[table] = []
        elif cfg.design[table] == procedure:
            result[table] = []
    return result

def mapData(procedure, chunksize=1000000):
    result = dataStructure(procedure)
    dimensions = []
    if procedure == "fact":
        dimensions = extractFKDimensions()
    print(f"[INFO] Initializing data processing for {procedure}s")
    for f in cfg.datasources:
        total_rows = 0
        for i, chunk in enumerate(
            pd.read_csv(f, chunksize=chunksize, low_memory=False), start=1
        ):
            result = mapRecords(chunk, result, dimensions=dimensions)
            total_rows += len(chunk)
            print(
                f"[INFO] File: {f} | Chunk {i} processed | Rows so far: {total_rows}"
            )
    print(f"[INFO] Data processing for {procedure}s concluded")  
    return cln.cleanData(result)
